{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ARAMCO_probabilistic.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "849b316592904ad999af48b783d14867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bb29ca14cc924fdab939759a8f1302c8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_65b4ad40a2fa487bb6344c6595a80c95",
              "IPY_MODEL_cde9cd2e34a04c968f7d432ed428b18d"
            ]
          }
        },
        "bb29ca14cc924fdab939759a8f1302c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65b4ad40a2fa487bb6344c6595a80c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_71188fd54f5f479aa2550ac79392a807",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 159,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 159,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17aca56522854a01ad2ac5095e757069"
          }
        },
        "cde9cd2e34a04c968f7d432ed428b18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9ee59d4ef6e54cd193a79c5549221153",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 159/159 [02:03&lt;00:00,  1.19it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5dbe766ca0774238844e66e6de0ffc8c"
          }
        },
        "71188fd54f5f479aa2550ac79392a807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17aca56522854a01ad2ac5095e757069": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ee59d4ef6e54cd193a79c5549221153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5dbe766ca0774238844e66e6de0ffc8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haimin777/aramco_2019/blob/master/ARAMCO_probabilistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-1ro2-DeYl5",
        "colab_type": "code",
        "outputId": "8b370926-bcad-4e51-eb20-572410850f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tQeJDysewN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR9Vv3XwezVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/AIgym/probability')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYzvUJxvduU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "from itertools import chain\n",
        "from skimage.io import imread, imshow, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dependency imports\n",
        "from absl import flags\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tf_probability.examples.models.bayesian_resnet import bayesian_resnet\n",
        "from tf_probability.examples.models.bayesian_vgg import bayesian_vgg\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "matplotlib.use(\"Agg\")\n",
        "from matplotlib import figure  \n",
        "from matplotlib.backends import backend_agg\n",
        "warnings.simplefilter(action=\"ignore\")\n",
        "tfd = tfp.distributions\n",
        "\n",
        "import seaborn as sns  \n",
        "HAS_SEABORN = True\n",
        "\n",
        "IMAGE_SHAPE = [128, 128, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5oDltCEqxbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get and resize train images based on avilable masks\n",
        "def get_data(path, train=True, num_classes=10, img_size=128):\n",
        "    ids = next(os.walk(path + \"images\"))[2]\n",
        "    X = np.zeros((len(ids), img_size, img_size, 1), dtype=np.float32)\n",
        "    if train:\n",
        "        #y = np.zeros((len(ids), img_size, img_size, num_classes), dtype=np.float32)\n",
        "        y = []\n",
        "    print('Getting and resizing images ... ')\n",
        "    for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
        "     \n",
        "        # Load images\n",
        "        img = load_img(path + 'images/' + id_, color_mode = \"grayscale\")\n",
        "        x_img = img_to_array(img)\n",
        "        x_img = cv2.equalizeHist(x_img.astype(np.uint8))\n",
        "        x_img = resize(x_img, (img_size, img_size, 1), mode='constant', preserve_range=True)\n",
        "\n",
        "        # calculate label based on mask\n",
        "        if train:\n",
        "            mask = np.amax(np.array(Image.open(path + 'masks/' + id_.split('_')[0]+'_xlinescustom.png')))\n",
        "            #mask = resize(mask, (128, 128, num_classes), mode='constant', preserve_range=True)\n",
        "            \n",
        "\n",
        "        # Save images\n",
        "        X[n, ..., 0] = x_img.squeeze() / 255\n",
        "        if train:\n",
        "            y.append([int(mask/255)])\n",
        "    print('Done!')\n",
        "    if train:\n",
        "        return X, np.array(y)\n",
        "    else:\n",
        "        return X\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M8t7tTYsDEk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "849b316592904ad999af48b783d14867",
            "bb29ca14cc924fdab939759a8f1302c8",
            "65b4ad40a2fa487bb6344c6595a80c95",
            "cde9cd2e34a04c968f7d432ed428b18d",
            "71188fd54f5f479aa2550ac79392a807",
            "17aca56522854a01ad2ac5095e757069",
            "9ee59d4ef6e54cd193a79c5549221153",
            "5dbe766ca0774238844e66e6de0ffc8c"
          ]
        },
        "outputId": "d71770bb-4638-4401-9dea-bec3ebb84333"
      },
      "source": [
        "X, y = get_data('/content/drive/My Drive/datasets/trap_seismic_dataset/', 128)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting and resizing images ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "849b316592904ad999af48b783d14867",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=159), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXPLzSUziMnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS = {\"learning_rate\": 0.0001,\n",
        "         \"epochs\": 1000,\n",
        "         \"batch_size\": 10,\n",
        "         \"data_dir\": os.path.join('/content/drive/My Drive/AIgym/probability',\n",
        "                                         \"bayesian_neural_network/data\"),\n",
        "\n",
        "         \"model_dir\": os.path.join('/content/drive/My Drive/AIgym/probability',\n",
        "                         \"bayesian_neural_network/\"),\n",
        "         \"eval_freq\": 400,\n",
        "         \"num_monte_carlo\": 50, #help Network draws to compute predictive probabilities.\n",
        "         \"architecture\": \"resnet\",\n",
        "         \"kernel_posterior_scale_mean\": -9.0, # help=\"Initial kernel posterior mean of the scale (log var) for q(w)\")\n",
        "         \"kernel_posterior_scale_constraint\": 0.2, #help=\"Posterior kernel constraint for the scale (log var) of q(w).\")\n",
        "         \"kl_annealing\": 50, #help=\"Epochs to anneal the KL term (anneals from 0 to 1)\")\n",
        "         \"subtract_pixel_mean\": True, #help=\"Boolean for normalizing the images\")\n",
        "         \"fake_data\": None, #help=\"If true, uses fake data. Defaults to real data.\")\n",
        "         \n",
        "          }\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBl6jnW1fcPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualicing and metrics\n",
        "\n",
        "\n",
        "def plot_weight_posteriors(names, qm_vals, qs_vals, fname):\n",
        "  \"\"\"Save a PNG plot with histograms of weight means and stddevs.\n",
        "  Args:\n",
        "    names: A Python `iterable` of `str` variable names.\n",
        "    qm_vals: A Python `iterable`, the same length as `names`,\n",
        "      whose elements are Numpy `array`s, of any shape, containing\n",
        "      posterior means of weight varibles.\n",
        "    qs_vals: A Python `iterable`, the same length as `names`,\n",
        "      whose elements are Numpy `array`s, of any shape, containing\n",
        "      posterior standard deviations of weight varibles.\n",
        "    fname: Python `str` filename to save the plot to.\n",
        "  \"\"\"\n",
        "  fig = figure.Figure(figsize=(10, 20))\n",
        "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
        "\n",
        "  ax = fig.add_subplot(1, 2, 1)\n",
        "  for n, qm in zip(names, qm_vals):\n",
        "    sns.distplot(qm.flatten(), ax=ax, label=n)\n",
        "  ax.set_title(\"weight means\")\n",
        "  ax.set_xlim([-1.5, 1.5])\n",
        "  ax.legend()\n",
        "\n",
        "  ax = fig.add_subplot(1, 2, 2)\n",
        "  for n, qs in zip(names, qs_vals):\n",
        "    sns.distplot(qs.flatten(), ax=ax)\n",
        "  ax.set_title(\"weight stddevs\")\n",
        "  ax.set_xlim([0, 1.])\n",
        "\n",
        "  fig.tight_layout()\n",
        "  canvas.print_figure(fname, format=\"png\")\n",
        "  print(\"saved {}\".format(fname))\n",
        "\n",
        "\n",
        "def plot_heldout_prediction(input_vals, probs,\n",
        "                            fname, n=2, title=\"\"):\n",
        "  \"\"\"Save a PNG plot visualizing posterior uncertainty on heldout data.\n",
        "  Args:\n",
        "    input_vals: A `float`-like Numpy `array` of shape\n",
        "      `[num_heldout] + IMAGE_SHAPE`, containing heldout input images.\n",
        "    probs: A `float`-like Numpy array of shape `[num_monte_carlo,\n",
        "      num_heldout, num_classes]` containing Monte Carlo samples of\n",
        "      class probabilities for each heldout sample.\n",
        "    fname: Python `str` filename to save the plot to.\n",
        "    n: Python `int` number of datapoints to vizualize.\n",
        "    title: Python `str` title for the plot.\n",
        "  \"\"\"\n",
        "  fig = figure.Figure(figsize=(10, 10*n))\n",
        "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
        "  for i in range(n):\n",
        "    ax = fig.add_subplot(n, 3, 3*i + 1)\n",
        "    ax.imshow(input_vals[i, :,:,0].reshape(IMAGE_SHAPE[:-1]), interpolation=\"None\")\n",
        "\n",
        "    ax = fig.add_subplot(n, 3, 3*i + 2)\n",
        "    for prob_sample in probs:\n",
        "      sns.barplot(np.arange(2), prob_sample[i, :], alpha=0.1, ax=ax) # was 10\n",
        "      ax.set_ylim([0, 1])\n",
        "    ax.set_title(\"posterior samples\")\n",
        "\n",
        "    ax = fig.add_subplot(n, 3, 3*i + 3)\n",
        "    sns.barplot(np.arange(2), np.mean(probs[:, i, :], axis=0), ax=ax) # was 10\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.set_title(\"predictive probs\")\n",
        "  fig.suptitle(title)\n",
        "  fig.tight_layout()\n",
        "\n",
        "  canvas.print_figure(fname, format=\"png\")\n",
        "  print(\"saved {}\".format(fname))\n",
        "\n",
        "\n",
        "def build_input_pipeline(x_train, x_test, y_train, y_test,\n",
        "                         batch_size, valid_size):\n",
        "  \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n",
        "\n",
        "  x_train = x_train.astype(\"float32\")\n",
        "  x_test = x_test.astype(\"float32\")\n",
        "\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  y_train = y_train.flatten()\n",
        "  y_test = y_test.flatten()\n",
        "\n",
        "  if FLAGS['subtract_pixel_mean']:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "  print(\"x_train shape:\" + str(x_train.shape))\n",
        "  print(str(x_train.shape[0]) + \" train samples\")\n",
        "  print(str(x_test.shape[0]) + \" test samples\")\n",
        "\n",
        "  # Build an iterator over training batches.\n",
        "  training_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "      (x_train, np.int32(y_train)))\n",
        "  training_batches = training_dataset.shuffle(\n",
        "      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
        "  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n",
        "\n",
        "  # Build a iterator over the heldout set with batch_size=heldout_size,\n",
        "  # i.e., return the entire heldout set as a constant.\n",
        "  heldout_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "      (x_test, np.int32(y_test)))\n",
        "  heldout_batches = heldout_dataset.repeat().batch(valid_size)\n",
        "  heldout_iterator = tf.compat.v1.data.make_one_shot_iterator(heldout_batches)\n",
        "\n",
        "  # Combine these into a feedable iterator that can switch between training\n",
        "  # and validation inputs.\n",
        "  handle = tf.compat.v1.placeholder(tf.string, shape=[])\n",
        "  feedable_iterator = tf.compat.v1.data.Iterator.from_string_handle(\n",
        "      handle, training_batches.output_types, training_batches.output_shapes)\n",
        "  images, labels = feedable_iterator.get_next()\n",
        "\n",
        "  return images, labels, handle, training_iterator, heldout_iterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4SfYLXphiqi",
        "colab_type": "code",
        "outputId": "340d3dad-08b8-4f8b-c6b4-92060fb421c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "\n",
        "#load data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=2018)\n",
        "\n",
        "(images, labels, handle, training_iterator, heldout_iterator) = build_input_pipeline(x_train, x_test, y_train, y_test,\n",
        "                                            FLAGS['batch_size'], 500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape:(135, 128, 128, 1)\n",
            "135 train samples\n",
            "24 test samples\n",
            "WARNING:tensorflow:From <ipython-input-12-18a40b33aebb>:112: DatasetV1.output_types (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_types(dataset)`.\n",
            "WARNING:tensorflow:From <ipython-input-12-18a40b33aebb>:112: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuoWV16zhyvP",
        "colab_type": "code",
        "outputId": "63bd146d-0a2d-4cca-b30a-08ef29583608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "model_fn = bayesian_resnet\n",
        "model = model_fn(\n",
        "      IMAGE_SHAPE,\n",
        "      num_classes=2,\n",
        "      kernel_posterior_scale_mean=FLAGS['kernel_posterior_scale_mean'],\n",
        "      kernel_posterior_scale_constraint=FLAGS['kernel_posterior_scale_constraint'])\n",
        "logits = model(images)\n",
        "labels_distribution = tfd.Categorical(logits=logits)\n",
        "\n",
        "  # Perform KL annealing. The optimal number of annealing steps\n",
        "  # depends on the dataset and architecture.\n",
        "t = tf.compat.v2.Variable(0.0)\n",
        "kl_regularizer = t / (FLAGS['kl_annealing'] * len(x_train) / FLAGS['batch_size'])\n",
        "\n",
        "  # Compute the -ELBO as the loss. The kl term is annealed from 0 to 1 over\n",
        "  # the epochs specified by the kl_annealing flag.\n",
        "log_likelihood = labels_distribution.log_prob(labels)\n",
        "neg_log_likelihood = -tf.reduce_mean(input_tensor=log_likelihood)\n",
        "kl = sum(model.losses) / len(x_train) * tf.minimum(1.0, kl_regularizer)\n",
        "loss = neg_log_likelihood + kl"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/layers/util.py:102: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4dpwRoxlhbm",
        "colab_type": "code",
        "outputId": "81782f87-ec75-499c-db24-9475fa535fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "  # Build metrics for evaluation. Predictions are formed from a single forward\n",
        "  # pass of the probabilistic layers. They are cheap but noisy\n",
        "  # predictions.\n",
        "  names = []\n",
        "  qmeans = []\n",
        "  qstds = []\n",
        "  for i, layer in enumerate(model.layers):\n",
        "    try:\n",
        "      q = layer.kernel_posterior\n",
        "    except AttributeError:\n",
        "      continue\n",
        "    names.append(\"Layer {}\".format(i))\n",
        "    qmeans.append(q.mean())\n",
        "    qstds.append(q.stddev())\n",
        "\n",
        "  predictions = tf.argmax(input=logits, axis=1)\n",
        "  with tf.compat.v1.name_scope(\"train\"):\n",
        "    train_accuracy, train_accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
        "        labels=labels, predictions=predictions)\n",
        "    opt = tf.compat.v1.train.AdamOptimizer(FLAGS['learning_rate'])\n",
        "    train_op = opt.minimize(loss)\n",
        "    update_step_op = tf.compat.v1.assign(t, t + 1)\n",
        "\n",
        "  with tf.compat.v1.name_scope(\"valid\"):\n",
        "    valid_accuracy, valid_accuracy_update_op = tf.compat.v1.metrics.accuracy(\n",
        "        labels=labels, predictions=predictions)\n",
        "\n",
        "  init_op = tf.group(tf.compat.v1.global_variables_initializer(),\n",
        "                     tf.compat.v1.local_variables_initializer())\n",
        "\n",
        "  stream_vars_valid = [\n",
        "      v for v in tf.compat.v1.local_variables() if \"valid/\" in v.name\n",
        "  ]\n",
        "  reset_valid_op = tf.compat.v1.variables_initializer(stream_vars_valid)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udIwrW18mdb2",
        "colab_type": "code",
        "outputId": "2a38c14a-c0c3-4b6f-b447-3b62d7b442fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "  \n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    # Run the training loop\n",
        "    train_handle = sess.run(training_iterator.string_handle())\n",
        "    heldout_handle = sess.run(heldout_iterator.string_handle())\n",
        "    training_steps = int(\n",
        "        round(FLAGS['epochs'] * (len(x_train) / FLAGS['batch_size'])))\n",
        "    for step in range(training_steps):\n",
        "      _ = sess.run([train_op,\n",
        "                    train_accuracy_update_op,\n",
        "                    update_step_op],\n",
        "                   feed_dict={handle: train_handle})\n",
        "\n",
        "      # Manually print the frequency\n",
        "      if step % 100 == 0:\n",
        "        loss_value, accuracy_value, kl_value = sess.run(\n",
        "            [loss, train_accuracy, kl], feed_dict={handle: train_handle})\n",
        "        print(\n",
        "            \"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f} KL: {:.3f}\".format(\n",
        "                step, loss_value, accuracy_value, kl_value))\n",
        "\n",
        "      if (step + 1) % FLAGS['eval_freq'] == 0:\n",
        "        # Compute log prob of heldout set by averaging draws from the model:\n",
        "        # p(heldout | train) = int_model p(heldout|model) p(model|train)\n",
        "        #                   ~= 1/n * sum_{i=1}^n p(heldout | model_i)\n",
        "        # where model_i is a draw from the posterior\n",
        "        # p(model|train).\n",
        "        probs = np.asarray([sess.run((labels_distribution.probs),\n",
        "                                     feed_dict={handle: heldout_handle})\n",
        "                            for _ in range(FLAGS['num_monte_carlo'])])\n",
        "        mean_probs = np.mean(probs, axis=0)\n",
        "\n",
        "        image_vals, label_vals = sess.run(\n",
        "            (images, labels), feed_dict={handle: heldout_handle})\n",
        "        heldout_lp = np.mean(np.log(mean_probs[np.arange(mean_probs.shape[0]),\n",
        "                                               label_vals.flatten()]))\n",
        "        print(\" ... Held-out nats: {:.3f}\".format(heldout_lp))\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        for _ in range(20):\n",
        "          sess.run(\n",
        "              valid_accuracy_update_op, feed_dict={handle: heldout_handle})\n",
        "        valid_value = sess.run(\n",
        "            valid_accuracy, feed_dict={handle: heldout_handle})\n",
        "\n",
        "        print(\n",
        "            \" ... Validation Accuracy: {:.3f}\".format(valid_value))\n",
        "        \n",
        "        qm_vals, qs_vals = sess.run((qmeans, qstds))\n",
        "        saver.save(sess, 'my_bayesian_model')\n",
        "\n",
        "        if HAS_SEABORN:\n",
        "          plot_weight_posteriors(names, qm_vals, qs_vals,\n",
        "                                 fname=os.path.join(\n",
        "                                     FLAGS['model_dir'],\n",
        "                                     \"step{:05d}_weights_equHist.png\".format(step)))\n",
        "\n",
        "          plot_heldout_prediction(image_vals, probs,\n",
        "                                  fname=os.path.join(\n",
        "                                      FLAGS['model_dir'],\n",
        "                                      \"step{:05d}_pred_equHist.png\".format(step)),\n",
        "                                  title=\"mean heldout logprob {:.2f}\"\n",
        "                                  .format(heldout_lp))\n",
        "\n",
        "        sess.run(reset_valid_op)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:   0 Loss: 1510.061 Accuracy: 0.500 KL: 472.952\n",
            "Step: 100 Loss: 47752.605 Accuracy: 0.546 KL: 47703.770\n",
            "Step: 200 Loss: 94884.477 Accuracy: 0.555 KL: 94765.672\n",
            "Step: 300 Loss: 141647.359 Accuracy: 0.547 KL: 141647.359\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}